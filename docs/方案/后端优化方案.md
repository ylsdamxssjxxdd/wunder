# 后端优化方案

## 1. 背景与目标

本方案基于对 `src` 下 Rust 后端核心链路的代码级审查，目标是：

- 提升高并发下的吞吐与尾延迟稳定性（重点关注 P95/P99）
- 降低请求路径中的阻塞点与锁竞争
- 降低数据库热点压力，避免随并发上升出现“雪崩式退化”
- 强化长时间运行稳定性（资源占用可预测、故障可恢复）

重点审查路径：

- 请求入口与流式链路：`/wunder`、`/wunder/chat/*`、WS/SSE
- 调度器：orchestrator（LLM 回合、工具执行、流事件）
- 存储层：SQLite/Postgres 统一接口与高频读写点
- 队列与后台循环：agent queue、channel outbox、monitor 持久化

---

## 2. 审查结论（优先级总览）

### P0（优先处理，收益最大）

1. **存储调用链同步化 + 大量 `spawn_blocking`**，导致上下文切换和线程池压力高。
2. **会话锁强依赖数据库轮询**，获取锁路径存在高频 `DELETE/COUNT`，并发时会成为热点。
3. **流事件持久化/恢复以轮询为主**，WS/SSE 客户端数量上来后数据库读放大明显。
4. **Agent 队列无全局并发上限与显式 claim 机制**，高峰时可能出现“瞬时任务洪峰”。
5. **写队列满载后回退同步写**，会把后台压力反弹到请求线程。

### P1（第二阶段）

6. 取消检测基于轮询 + 监控状态全局互斥，存在额外 CPU 与锁竞争。
7. 登录密码校验（Argon2）在 async 路径直接执行，可能阻塞运行时。
8. Channel outbox 逐条串行发送，吞吐上限偏低。
9. 多处大对象 clone（消息列表/JSON）增加 CPU 与内存压力。
10. 长期运行下部分全局缓存/锁映射增长缺少回收策略。

---

## 3. 关键瓶颈与优化方案

## 3.1 存储调用模型：从“同步 + 包裹异步”改为“原生异步热路径”

### 现状证据

- Postgres 层使用 `block_on + block_in_place`：`src/storage/postgres.rs:159`、`src/storage/postgres.rs:164`
- 大量调用侧使用 `spawn_blocking` 包裹存储操作，如：
  - `src/orchestrator/execute.rs:46`
  - `src/orchestrator/execute.rs:70`
  - `src/api/chat.rs:709`
  - `src/channels/service.rs:1825`

### 问题

- 每个高频读写点都经历“async → blocking → async”切换
- 并发升高时，blocking 线程池成为隐性瓶颈
- 端到端延迟抖动（尤其 P99）会明显放大

### 方案

- 新增 `AsyncStorageBackend`（仅覆盖高频路径）：
  - session_lock、stream_events、agent_tasks、outbox、chat append/load
- 低频管理类接口保留同步实现，分阶段迁移
- Postgres 热路径直接使用 async client，去掉热路径 `block_in_place`
- 调用侧删除对应 `spawn_blocking`

### 预期收益

- 降低上下文切换开销
- 提升高并发稳定性与尾延迟
- 为后续批量写入/流水线查询提供基础

---

## 3.2 会话锁限流：减少数据库热点与轮询风暴

### 现状证据

- 限流器循环获取锁：`src/orchestrator/limiter.rs:52`
- 获取锁时执行：
  - 过期清理：`src/storage/postgres.rs:2043`
  - 全局计数：`src/storage/postgres.rs:2070`

### 问题

- 每次尝试都可能触发表级高频操作
- 用户并发冲突时，轮询会持续放大数据库压力

### 方案

- 两级限流：
  - 进程内 `Semaphore`（快路径）
  - 数据库锁（跨实例一致性）
- 过期清理从“获取锁路径内执行”改为后台定时任务
- `COUNT(*)` 改为缓存计数/近似计数，或改成分区计数策略
- 轮询退避加入随机抖动（jitter）避免同频冲击

### 预期收益

- 锁热点明显下降
- 高并发下数据库更稳定

---

## 3.3 流事件（SSE/WS）：从“轮询拉取”向“内存广播 + 批量落库”演进

### 现状证据

- 每个事件持久化时可能触发 blocking 写：`src/orchestrator/event_stream.rs:275`
- WS 恢复逻辑周期性 `spawn_blocking + load_stream_events`：`src/api/ws_helpers.rs:466`
- Chat resume 也使用轮询：`src/api/chat.rs:709`

### 问题

- 连接数上来后，轮询会线性推高数据库 QPS
- delta 事件频繁落库，写放大明显

### 方案

- 引入会话级内存事件总线（broadcast/ring buffer）：
  - 在线客户端优先走内存推送
  - 仅断线重连场景回补数据库
- 引入 `StreamEventWriter` 批量写：
  - 批大小 + flush 周期双阈值
  - 按 session 分桶写入
- TTL 清理改为独立后台任务，不在事件写入路径触发
- 保留 SSE 兜底，但默认链路优先 WS

### 预期收益

- 大幅降低轮询读与小包写
- 提升实时流稳定性与并发上限

---

## 3.4 Agent 队列：增加并发闸门与任务 claim 机制

### 现状证据

- 周期轮询并直接批量 `spawn`：`src/services/agent_runtime.rs:596`、`src/services/agent_runtime.rs:629`
- 当前配置无 worker 上限，仅有重试与轮询间隔：`src/core/config.rs:150`

### 问题

- 高峰时可能瞬时拉起过多任务
- 多实例部署时缺少明确 claim，存在重复消费风险

### 方案

- 新增配置：`agent_queue.max_workers`
- 使用 `Semaphore` 限制并发执行数
- 存储层新增 claim 接口（Postgres 可用 `FOR UPDATE SKIP LOCKED`）
- 轮询改“事件唤醒优先 + 低频兜底轮询”

### 预期收益

- 队列执行更平稳
- 并发可控，降低尾延迟抖动

---

## 3.5 写入队列降级策略：避免回退同步写打爆请求线程

### 现状证据

- workspace 写队列满后直接同步写：`src/services/workspace.rs:132`、`src/services/workspace.rs:136`
- monitor 写队列满也有同步回退：`src/ops/monitor.rs:195`、`src/ops/monitor.rs:199`

### 问题

- 后台压力会反向传导到请求链路
- 高峰期延迟剧烈抖动

### 方案

- 增加可配置溢出策略：`drop_latest / drop_oldest / block_with_timeout`
- 关键日志与普通日志分级队列（关键数据优先）
- 批量写入（N 条或 T 毫秒 flush）
- 暴露队列水位指标并设置告警

### 预期收益

- 防止高峰时“队列挤爆导致主流程卡顿”

---

## 3.6 取消机制：从轮询改为会话级通知

### 现状证据

- 取消检查固定 200ms 轮询：`src/orchestrator/llm.rs:114`
- 取消状态读取依赖监控状态互斥：`src/ops/monitor.rs:976`

### 方案

- 为 session 引入 `CancellationToken/Notify`
- 将 `wait_for_cancelled` 改为事件驱动等待，减少无效轮询
- monitor 保留审计状态，但不承担实时取消通知

---

## 3.7 登录链路 CPU 隔离

### 现状证据

- 登录在 async handler 中直接调用：`src/api/auth.rs:95`
- 密码校验为 CPU 密集：`src/services/user_store.rs:159`

### 方案

- 登录/注册/改密哈希计算统一放入专用 blocking 池
- 增加登录限速与失败熔断，防止恶意高频压测

---

## 3.8 Channel Outbox 并发发送

### 现状证据

- outbox loop 逐条串行处理：`src/channels/service.rs:1287`

### 方案

- 改为“按 channel/account 分组 + 有界并发”发送
- 增加 claim 更新（防重复投递）
- 对失败重试按 provider 做退避策略模板化

---

## 3.9 减少大对象 clone 与 JSON 热路径开销

### 现状证据

- 每轮记录日志前 clone 全量消息：`src/orchestrator/execute.rs:262`

### 方案

- 日志脱敏与摘要化使用引用处理，避免整包 clone
- 高频 JSON 构建改为复用 buffer（可逐步引入）
- 工具结果长文本截断提前做，避免重复序列化

---

## 3.10 长期稳定性：全局结构回收策略

### 现状证据

- 文档级锁映射常驻增长：`src/services/vector_knowledge.rs:674`

### 方案

- 为全局 map 增加 TTL/LRU 清理
- 定期输出内存热点对象统计
- 纳入长稳压测验收（72h/7d）

---

## 4. 分阶段落地计划

## 阶段 A（1~3 天，低风险快收益）

- 增加 `agent_queue.max_workers` 并接入 `Semaphore`
- 将登录/密码哈希搬到 `spawn_blocking` 专用路径
- 调整流事件清理为后台任务，不在写路径触发
- 增加关键指标：队列水位、锁等待、stream 轮询次数

## 阶段 B（3~7 天，中风险高收益）

- 落地 StreamEventWriter 批量落库
- 引入会话内存广播，在线流式优先内存派发
- 会话锁路径改造（去掉热路径 `COUNT/DELETE`）

## 阶段 C（7~14 天，架构级优化）

- 建立 AsyncStorageBackend 热路径接口并迁移 orchestrator/chat/ws/channel
- Agent/Outbox 引入 claim 语义（支持多实例稳定并发）
- 收敛 `spawn_blocking` 调用面，建立统一规范

## 阶段 D（稳定性与验收）

- 压测 + 混沌演练 + 长稳运行观测
- 完成回归基线与报警阈值固化

---

## 5. 建议新增指标（必须）

- 请求维度：`/wunder` P50/P95/P99、错误率、超时率
- 存储维度：连接池等待时长、慢查询数、每秒写入条数
- 队列维度：agent/outbox 队列长度、排队时长、重试率
- 流式维度：活跃 WS/SSE 数、每会话事件速率、回补读频次
- 运行时维度：tokio blocking 池繁忙度、内存增长率、线程数

---

## 6. 压测与验收标准（建议）

- **场景 S1：`/wunder` 流式 300 并发（混合短问答+工具调用）**
  - 目标：服务端非 LLM 开销 P95 降低 30%+
- **场景 S2：WS 在线 3000 连接 + 500 活跃流**
  - 目标：数据库轮询读下降 60%+
- **场景 S3：Agent 队列积压 1 万任务**
  - 目标：无任务重复执行；吞吐稳定，无长时间抖动
- **场景 S4：72 小时长稳**
  - 目标：无明显内存泄漏；无队列失控；无频繁重启

---

## 7. 风险与回滚策略

- 所有高风险改造必须加开关（feature flag / config）
- 流式链路改造遵循“WS 新链路 + SSE 旧链路兜底”
- 存储异步化按模块迁移，允许灰度回退到旧实现
- 每阶段必须有可量化基线，不达标不进入下一阶段

---

## 8. 近期执行清单（建议本周）

1. 落地 `agent_queue.max_workers` + 并发闸门
2. 会话锁热点降温（先拆出过期清理任务）
3. StreamEventWriter 批量写入 PoC
4. 登录哈希迁移到 blocking 专用执行
5. 增补可观测指标与 dashboard（先覆盖 P0 链路）

> 备注：以上方案优先保证“高并发稳定性”和“尾延迟收敛”，再逐步追求峰值吞吐。
