# 性能优化

## 2026-01-24 工具路径根目录缓存

### 背景
- 管理端性能面板的 file_ops 在高并发（300）场景出现明显延迟，影响文件相关工具的用户体验。

### 问题定位
- 文件工具会在每次调用时收集 allow_paths 与技能目录并做路径去重/归一化。
- 这一步包含 fs::canonicalize 与路径比较，在高并发下重复执行，会放大路径解析与 I/O 开销。
- 由于允许路径和技能根目录在单次请求内基本不变，重复计算属于非必要开销。

### 优化方案
- 新增 ToolRoots 缓存，将 allow_roots/read_roots 预先计算并挂载到 ToolContext。
- collect_allow_roots/collect_read_roots 优先复用缓存，缺失时回退到旧逻辑，保证兼容性。
- 在 orchestrator 与性能采样入口统一构建并复用缓存，减少每次工具调用的路径归一化成本。

### 效果（本地 Docker 实测）
- file_ops（300 并发）平均耗时从约 1.6s 降至约 28ms。
- 实际收益与机器负载、文件系统类型相关，但对高并发文件工具场景有明显改善。

### 相关改动
- 缓存定义与构建：src/tools.rs:75
- 缓存接入工具上下文：src/orchestrator/execute.rs:82
- 性能采样复用缓存：src/performance.rs:7

### 指导与注意事项
- 新增创建 ToolContext 的入口时，应通过 build_tool_roots 注入 allow_roots/read_roots，避免退化到每次调用重复计算。
- 配置层面尽量收敛 security.allow_paths，避免无用路径触发多余的 canonicalize。
- 性能回归验证建议使用管理端性能面板或接口：/wunder/admin/performance/sample。
- 如需扩展新的根路径来源，优先在 build_tool_roots 内统一处理，保持单点缓存逻辑。

## 2026-01-24 沙盒服务 Rust 化

### 背景
- command_exec 在高并发场景出现排队与尾延迟抖动，Python 沙盒服务的 worker/线程池调度开销明显。

### 优化方案
- 共享沙盒由 `wunder-server` 的 `sandbox` 模式直接提供，独立容器运行并复用相同镜像与挂载。
- 沙盒服务使用 `tokio::process::Command` 异步执行命令与 ptc，减少 Python 事件循环与线程池开销。
- ptc 仍通过容器内 `python3` 执行脚本，保持能力不变，同时移除 `app/` 旧代码。

### 相关改动
- 沙盒入口：`src/sandbox_server.rs`
- 服务模式切换：`src/main.rs` + `src/config.rs`
- 部署配置：`docker-compose.rust.x86.yml`、`docker-compose.rust.arm.yml`

### 指导与注意事项
- 沙盒容器需设置 `WUNDER_SERVER_MODE=sandbox` + `WUNDER_PORT=9001`，并确保 `WUNDER_SANDBOX_ENDPOINT` 指向容器内地址（默认 `http://sandbox:9001`）。
- 建议使用 release 构建运行沙盒服务，并优先使用本地磁盘卷，避免网络卷导致 I/O 抖动。
- 若 `command_exec` 仍有排队，可考虑提升宿主机 CPU/IO 资源或拆分沙盒实例。
- 指标验证建议使用管理端性能面板或接口：/wunder/admin/performance/sample。
