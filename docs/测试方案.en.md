# wunder API Test Plan (Practical)

## 1. Goals and scope
- Functionality: `/wunder` entry, SSE streaming protocol, tool list, system prompt, temp workspace, monitor APIs.
- Stability: long soak, error rate, resource leak trends (CPU/memory/handles).
- Performance: P50/P95/P99 latency, throughput, error rate, peak recovery time.

## 2. Environment setup
1) Start backend service:
```
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

2) Install dependencies:
```
pip install -r requirements.txt
```
Note: add a requirements-dev.txt if you need extra dev dependencies.

3) Install k6 (performance/stability testing):
- Windows: use the official package or installer (see k6 docs).

## 3. Functional testing (pytest)
### 3.1 Coverage
- `/wunder` non-stream: fields and content returned correctly.
- `/wunder` SSE: progress/llm_output/final events are complete and final answer parses.
- Concurrency guard: same user_id returns 429.
- `/wunder/system_prompt`: prompt returns.
- `/wunder/tools`: schema and tool list returned.
- `/wunder/workspace`: upload/list/download/delete loop works.
- `/wunder/admin/monitor`: system resource fields returned.

### 3.2 Command
```
pytest
```

Note: tests use FakeLLMClient to avoid real model calls and networking.

## 4. Performance testing (k6 load)
### 4.1 Load script
- `tests/performance/k6_wunder.js`: unified script, choose quick/load/spike/soak via `WUNDER_PROFILE`.
- quick: single VU, 10s, optional fixed user_id to reduce workspace leftovers
- load: staged ramp 5→20→0, supports `WUNDER_STREAM`, validates answer in non-stream
- spike: burst traffic (up to 80 VU) to observe shock and recovery
- soak: fixed VU for long duration, looser thresholds with stability focus

### 4.2 Commands
```
k6 run -e WUNDER_PROFILE=load tests/performance/k6_wunder.js
k6 run -e WUNDER_PROFILE=spike tests/performance/k6_wunder.js
k6 run -e WUNDER_PROFILE=quick tests/performance/k6_wunder.js
```

Docker run (no local k6 install):
```
docker run --rm -i -e WUNDER_API_KEY=your-key -v "${PWD}:/work" -w /work grafana/k6 run -e WUNDER_PROFILE=load -e WUNDER_BASE_URL=http://192.168.0.2:8000/wunder tests/performance/k6_wunder.js
```

### 4.3 Optional parameters (env vars)
- `WUNDER_PROFILE`: load profile, `quick/load/spike/soak`, default `load`
- `WUNDER_BASE_URL`: default `http://127.0.0.1:8000/wunder`
- `WUNDER_API_KEY`: API key (required if `security.api_key` is enabled)
- `WUNDER_USER_PREFIX`: user prefix for multi-user simulation
- `WUNDER_QUESTION`: question text
- `WUNDER_STREAM`: whether to stream (load only, default false)
- `WUNDER_USER_ID`: fixed user_id (quick only)
- `WUNDER_SOAK_VUS`: VUs for soak
- `WUNDER_SOAK_DURATION`: soak duration

Example:
```
$env:WUNDER_API_KEY="your-key"; $env:WUNDER_BASE_URL="http://127.0.0.1:8000/wunder"; $env:WUNDER_PROFILE="load"; k6 run tests/performance/k6_wunder.js
```

## 5. Stability testing (k6 Soak)
### 5.1 Soak script
- `tests/performance/k6_wunder.js` (`WUNDER_PROFILE=soak`)

### 5.2 Command
```
$env:WUNDER_PROFILE="soak"; $env:WUNDER_SOAK_VUS="10"; $env:WUNDER_SOAK_DURATION="30m"; k6 run tests/performance/k6_wunder.js
```

### 5.3 Observations
- Error rate (http_req_failed) trend.
- Latency percentiles (p95/p99) drift over time.
- Server CPU/memory trends (upward drift indicates leaks).

## 6. Metrics collection tips
- Server: `/wunder/admin/monitor` for CPU/memory metrics.
- Logs: `data/wunder.db` (`system_logs` table).
- OS metrics: use Grafana/Prometheus if available.

## 7. Regression & acceptance
- Define SLO targets (e.g. P95 latency < 2s, error rate < 1%).
- Run nightly soak; run functional tests + baseline load tests after core changes.

## 8. Evaluation tests (case set)
### 8.1 Cases
- `tests/evaluation/cases/basic.yaml`: basic eval cases for non-stream and SSE structure checks.
- The evaluation script auto-generates user_id to avoid conflicts.

### 8.2 Command
```
python scripts/run_evaluation.py --base-url http://127.0.0.1:8000/wunder
```

### 8.3 Output
- Default output: `data/eval_reports/eval_report_YYYYMMDD_HHMMSS.json`.
- Use `--output` to set a custom path.

## 9. Cleanup test artifacts
### 9.1 Cleanup workspace and history
```
python scripts/cleanup_test_data.py
```

### 9.2 Local cleanup only or remove reports
```
python scripts/cleanup_test_data.py --no-api --remove-reports
```
